{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bits to Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits =\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 0. 0. 1. 0. 1.]\n",
      " [1. 0. 1. 1. 1. 1. 1. 1.]], shape=(2, 8), dtype=float32)\n",
      "new_shape =\n",
      " [-1, 2, 4]\n",
      "bits_reshaped =\n",
      " tf.Tensor(\n",
      "[[[1 1 1 0]\n",
      "  [0 1 0 1]]\n",
      "\n",
      " [[1 0 1 1]\n",
      "  [1 1 1 1]]], shape=(2, 2, 4), dtype=int32)\n",
      "binary_base =\n",
      " tf.Tensor([8 4 2 1], shape=(4,), dtype=int32)\n",
      "int_rep =\n",
      " tf.Tensor(\n",
      "[[14  5]\n",
      " [11 15]], shape=(2, 2), dtype=int32)\n",
      "tf.Tensor(\n",
      "[[-0.9486833-0.3162278j  0.3162278-0.9486833j]\n",
      " [-0.9486833+0.9486833j -0.9486833-0.9486833j]], shape=(2, 2), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# Imports & Basics\n",
    "\n",
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sionna.utils.misc import hard_decisions\n",
    "from sionna.utils.metrics import compute_ber, compute_ser\n",
    "\n",
    "CODERATE = 0.5\n",
    "n = 16\n",
    "k = int(n*CODERATE)\n",
    "\n",
    "NUM_BITS_PER_SYMBOL = 4 # QPSK\n",
    "BLOCK_LENGTH = k\n",
    "BATCH_SIZE = 2 # How many examples are processed by Sionna in parallel\n",
    "EBN0_DB_MIN = -10.0 # Minimum value of Eb/N0 [dB] for simulations\n",
    "EBN0_DB_MAX = 10.0 # Maximum value of Eb/N0 [dB] for simulations\n",
    "\n",
    "# Binary source\n",
    "binary_source = sn.utils.BinarySource()\n",
    "\n",
    "# Constellation\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "bits = binary_source([BATCH_SIZE,BLOCK_LENGTH])\n",
    "print('bits =\\n',bits)\n",
    "\n",
    "new_shape = [-1] + bits.shape[1:-1].as_list() + \\\n",
    "           [int(bits.shape[-1] / NUM_BITS_PER_SYMBOL),\n",
    "            NUM_BITS_PER_SYMBOL]\n",
    "print('new_shape =\\n',new_shape)\n",
    "\n",
    "bits_reshaped = tf.cast(tf.reshape(bits, new_shape), tf.int32)\n",
    "print('bits_reshaped =\\n',bits_reshaped)\n",
    "\n",
    "binary_base = 2**tf.constant(\n",
    "                        range(NUM_BITS_PER_SYMBOL-1,-1,-1))\n",
    "print('binary_base =\\n',binary_base)\n",
    "\n",
    "int_rep = tf.reduce_sum(bits_reshaped * binary_base, axis=-1)\n",
    "print('int_rep =\\n',int_rep)\n",
    "\n",
    "x = tf.gather(constellation.points, int_rep, axis=0)\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EBN0_DB to EBN0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EBN0_DB = -3.0\n",
      "no = tf.Tensor(0.4988156, shape=(), dtype=float32)\n",
      "EBN0_DB = -2.6842105263157894\n",
      "no = tf.Tensor(0.46383238, shape=(), dtype=float32)\n",
      "EBN0_DB = -2.3684210526315788\n",
      "no = tf.Tensor(0.43130267, shape=(), dtype=float32)\n",
      "EBN0_DB = -2.0526315789473686\n",
      "no = tf.Tensor(0.4010543, shape=(), dtype=float32)\n",
      "EBN0_DB = -1.736842105263158\n",
      "no = tf.Tensor(0.37292734, shape=(), dtype=float32)\n",
      "EBN0_DB = -1.4210526315789473\n",
      "no = tf.Tensor(0.346773, shape=(), dtype=float32)\n",
      "EBN0_DB = -1.105263157894737\n",
      "no = tf.Tensor(0.32245293, shape=(), dtype=float32)\n",
      "EBN0_DB = -0.7894736842105265\n",
      "no = tf.Tensor(0.29983848, shape=(), dtype=float32)\n",
      "EBN0_DB = -0.47368421052631593\n",
      "no = tf.Tensor(0.27881005, shape=(), dtype=float32)\n",
      "EBN0_DB = -0.1578947368421053\n",
      "no = tf.Tensor(0.2592564, shape=(), dtype=float32)\n",
      "EBN0_DB = 0.1578947368421053\n",
      "no = tf.Tensor(0.2410741, shape=(), dtype=float32)\n",
      "EBN0_DB = 0.4736842105263155\n",
      "no = tf.Tensor(0.22416694, shape=(), dtype=float32)\n",
      "EBN0_DB = 0.7894736842105261\n",
      "no = tf.Tensor(0.20844556, shape=(), dtype=float32)\n",
      "EBN0_DB = 1.1052631578947363\n",
      "no = tf.Tensor(0.19382674, shape=(), dtype=float32)\n",
      "EBN0_DB = 1.421052631578947\n",
      "no = tf.Tensor(0.18023318, shape=(), dtype=float32)\n",
      "EBN0_DB = 1.7368421052631575\n",
      "no = tf.Tensor(0.16759297, shape=(), dtype=float32)\n",
      "EBN0_DB = 2.052631578947368\n",
      "no = tf.Tensor(0.15583925, shape=(), dtype=float32)\n",
      "EBN0_DB = 2.3684210526315788\n",
      "no = tf.Tensor(0.14490984, shape=(), dtype=float32)\n",
      "EBN0_DB = 2.6842105263157894\n",
      "no = tf.Tensor(0.13474695, shape=(), dtype=float32)\n",
      "EBN0_DB = 3.0\n",
      "no = tf.Tensor(0.1252968, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Imports & Basics\n",
    "\n",
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sionna.utils.misc import hard_decisions\n",
    "from sionna.utils.metrics import compute_ber, compute_ser\n",
    "\n",
    "NUM_BITS_PER_SYMBOL = 4 # QPSK\n",
    "BLOCK_LENGTH = 8\n",
    "BATCH_SIZE = 2\n",
    "EBN0_DB_MIN = -3.0\n",
    "EBN0_DB_MAX = 3.0\n",
    "\n",
    "for EBN0_DB in np.linspace(EBN0_DB_MIN,EBN0_DB_MAX,20):\n",
    "    print('EBN0_DB =',EBN0_DB)\n",
    "    no = sn.utils.ebnodb2no(ebno_db=EBN0_DB,\n",
    "                            num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                            coderate=1.0)\n",
    "    print('no =',no)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sionna.channel.AWGN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits =\n",
      " tf.Tensor(\n",
      "[[1. 1. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 0. 1. 1. 1.]], shape=(2, 8), dtype=float32)\n",
      "no = tf.Tensor(2.5, shape=(), dtype=float32)\n",
      "x =\n",
      " tf.Tensor(\n",
      "[[-0.9486833-0.9486833j -0.3162278+0.3162278j]\n",
      " [ 0.9486833-0.9486833j  0.9486833-0.9486833j]], shape=(2, 2), dtype=complex64)\n",
      "noise =\n",
      " tf.Tensor(\n",
      "[[-0.2457148 -0.4940561j  -0.2502328 +0.0766773j ]\n",
      " [ 0.6659922 -0.00514933j  0.40934482-0.22090244j]], shape=(2, 2), dtype=complex64)\n",
      "no = tf.Tensor([[2.5]], shape=(1, 1), dtype=float32)\n",
      "no = tf.Tensor([[2.5]], shape=(1, 1), dtype=float32)\n",
      "noise =\n",
      " tf.Tensor(\n",
      "[[-0.3885092 -0.7811713j  -0.39565277+0.12123746j]\n",
      " [ 1.0530262 -0.00814181j  0.647231  -0.34927744j]], shape=(2, 2), dtype=complex64)\n"
     ]
    }
   ],
   "source": [
    "# Imports & Basics\n",
    "\n",
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sionna.utils import expand_to_rank, complex_normal\n",
    "\n",
    "NUM_BITS_PER_SYMBOL = 4 # QPSK\n",
    "BLOCK_LENGTH = 8\n",
    "BATCH_SIZE = 2\n",
    "EBN0_DB = -10\n",
    "\n",
    "# Binary source\n",
    "binary_source = sn.utils.BinarySource()\n",
    "\n",
    "bits = binary_source([BATCH_SIZE,BLOCK_LENGTH])\n",
    "print('bits =\\n',bits)\n",
    "\n",
    "no = sn.utils.ebnodb2no(ebno_db=EBN0_DB,\n",
    "                        num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                        coderate=1.0)\n",
    "print('no =',no)\n",
    "\n",
    "# Constellation\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "#constellation.show(figsize=(7,7));\n",
    "\n",
    "# Mapper and Demapper\n",
    "mapper = sn.mapping.Mapper(constellation=constellation)\n",
    "\n",
    "x = mapper(bits)\n",
    "print('x =\\n',x)\n",
    " \n",
    "# Create tensors of real-valued Gaussian noise for each complex dim.\n",
    "noise = complex_normal(tf.shape(x), dtype=x.dtype)\n",
    "print('noise =\\n',noise)\n",
    "\n",
    "# Add extra dimensions for broadcasting\n",
    "no = expand_to_rank(no, tf.rank(x), axis=-1)\n",
    "print('no =',no)\n",
    "\n",
    "# Apply variance scaling\n",
    "real_dtype = tf.dtypes.as_dtype(tf.complex64).real_dtype\n",
    "no = tf.cast(no, real_dtype)\n",
    "print('no =',no)\n",
    "noise *= tf.cast(tf.sqrt(no), noise.dtype)\n",
    "print('noise =\\n',noise)\n",
    "\n",
    "# Add noise to input\n",
    "y = x + noise\n",
    "#print('y = x + noise =\\n',y)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# logits2llrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits2llrs =\n",
      " None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/im/Documents/GitHub/sionna/Jupyter Notebooks/Test.ipynb 单元格 4\u001b[0m in \u001b[0;36m<cell line: 271>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=265'>266</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mlogits2llrs =\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m,logits2llrs)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=267'>268</a>\u001b[0m \u001b[39m# if self._with_prior:\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=268'>269</a>\u001b[0m \u001b[39m#     llr = self._logits2llrs([exponents, prior])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=269'>270</a>\u001b[0m \u001b[39m# else:\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=270'>271</a>\u001b[0m llr \u001b[39m=\u001b[39m logits2llrs(exponents)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=272'>273</a>\u001b[0m \u001b[39m# Reshape LLRs to [...,n*num_bits_per_symbol]\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=273'>274</a>\u001b[0m out_shape \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mconcat([tf\u001b[39m.\u001b[39mshape(y)[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=274'>275</a>\u001b[0m                         [y\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \\\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/im/Documents/GitHub/sionna/Jupyter%20Notebooks/Test.ipynb#W4sZmlsZQ%3D%3D?line=275'>276</a>\u001b[0m                         constellation\u001b[39m.\u001b[39mnum_bits_per_symbol]], \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "# Imports & Basics\n",
    "\n",
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def SymbolLogits2LLRs(method, num_bits_per_symbol, hard_out=False, with_prior=False, dtype=tf.float32, **kwargs):\n",
    "    # pylint: disable=line-too-long\n",
    "    r\"\"\"\n",
    "    SymbolLogits2LLRs(method, num_bits_per_symbol, hard_out=False, with_prior=False, dtype=tf.float32, **kwargs)\n",
    "\n",
    "    Computes log-likelihood ratios (LLRs) or hard-decisions on bits\n",
    "    from a tensor of logits (i.e., unnormalized log-probabilities) on constellation points.\n",
    "    If the flag ``with_prior`` is set, prior knowledge on the bits is assumed to be available.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    method : One of [\"app\", \"maxlog\"], str\n",
    "        The method used for computing the LLRs.\n",
    "\n",
    "    num_bits_per_symbol : int\n",
    "        The number of bits per constellation symbol, e.g., 4 for QAM16.\n",
    "\n",
    "    hard_out : bool\n",
    "        If `True`, the layer provides hard-decided bits instead of soft-values.\n",
    "        Defaults to `False`.\n",
    "\n",
    "    with_prior : bool\n",
    "        If `True`, it is assumed that prior knowledge on the bits is available.\n",
    "        This prior information is given as LLRs as an additional input to the layer.\n",
    "        Defaults to `False`.\n",
    "\n",
    "    dtype : One of [tf.float32, tf.float64] tf.DType (dtype)\n",
    "        The dtype for the input and output.\n",
    "        Defaults to `tf.float32`.\n",
    "\n",
    "    Input\n",
    "    -----\n",
    "    logits or (logits, prior):\n",
    "        Tuple:\n",
    "\n",
    "    logits : [...,n, num_points], tf.float\n",
    "        Logits on constellation points.\n",
    "\n",
    "    prior : [num_bits_per_symbol] or [...n, num_bits_per_symbol], tf.float\n",
    "        Prior for every bit as LLRs.\n",
    "        It can be provided either as a tensor of shape `[num_bits_per_symbol]`\n",
    "        for the entire input batch, or as a tensor that is \"broadcastable\"\n",
    "        to `[..., n, num_bits_per_symbol]`.\n",
    "        Only required if the ``with_prior`` flag is set.\n",
    "\n",
    "    Output\n",
    "    ------\n",
    "    : [...,n, num_bits_per_symbol], tf.float\n",
    "        LLRs or hard-decisions for every bit.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    With the \"app\" method, the LLR for the :math:`i\\text{th}` bit\n",
    "    is computed according to\n",
    "\n",
    "    .. math::\n",
    "        LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=1\\lvert \\mathbf{z},\\mathbf{p}\\right)}{\\Pr\\left(b_i=0\\lvert \\mathbf{z},\\mathbf{p}\\right)}\\right) =\\ln\\left(\\frac{\n",
    "                \\sum_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n",
    "                e^{z_c}\n",
    "                }{\n",
    "                \\sum_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n",
    "                e^{z_c}\n",
    "                }\\right)\n",
    "\n",
    "    where :math:`\\mathcal{C}_{i,1}` and :math:`\\mathcal{C}_{i,0}` are the\n",
    "    sets of :math:`2^K` constellation points for which the :math:`i\\text{th}` bit is\n",
    "    equal to 1 and 0, respectively. :math:`\\mathbf{z} = \\left[z_{c_0},\\dots,z_{c_{2^K-1}}\\right]` is the vector of logits on the constellation points, :math:`\\mathbf{p} = \\left[p_0,\\dots,p_{K-1}\\right]`\n",
    "    is the vector of LLRs that serves as prior knowledge on the :math:`K` bits that are mapped to\n",
    "    a constellation point and is set to :math:`\\mathbf{0}` if no prior knowledge is assumed to be available,\n",
    "    and :math:`\\Pr(c\\lvert\\mathbf{p})` is the prior probability on the constellation symbol :math:`c`:\n",
    "\n",
    "    .. math::\n",
    "        \\Pr\\left(c\\lvert\\mathbf{p}\\right) = \\prod_{k=0}^{K-1} \\Pr\\left(b_k = \\ell(c)_k \\lvert\\mathbf{p} \\right)\n",
    "        = \\prod_{k=0}^{K-1} \\text{sigmoid}\\left(p_k \\ell(c)_k\\right)\n",
    "\n",
    "    where :math:`\\ell(c)_k` is the :math:`k^{th}` bit label of :math:`c`, where 0 is\n",
    "    replaced by -1.\n",
    "    The definition of the LLR has been\n",
    "    chosen such that it is equivalent with that of logits. This is\n",
    "    different from many textbooks in communications, where the LLR is\n",
    "    defined as :math:`LLR(i) = \\ln\\left(\\frac{\\Pr\\left(b_i=0\\lvert y\\right)}{\\Pr\\left(b_i=1\\lvert y\\right)}\\right)`.\n",
    "\n",
    "    With the \"maxlog\" method, LLRs for the :math:`i\\text{th}` bit\n",
    "    are approximated like\n",
    "\n",
    "    .. math::\n",
    "        \\begin{align}\n",
    "            LLR(i) &\\approx\\ln\\left(\\frac{\n",
    "                \\max_{c\\in\\mathcal{C}_{i,1}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n",
    "                    e^{z_c}\n",
    "                }{\n",
    "                \\max_{c\\in\\mathcal{C}_{i,0}} \\Pr\\left(c\\lvert\\mathbf{p}\\right)\n",
    "                    e^{z_c}\n",
    "                }\\right)\n",
    "                .\n",
    "        \\end{align}\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 method,\n",
    "                 num_bits_per_symbol,\n",
    "                 hard_out=False,\n",
    "                 with_prior=False,\n",
    "                 dtype=tf.float32,\n",
    "                 **kwargs):\n",
    "        super().__init__(dtype=dtype, **kwargs)\n",
    "        assert method in (\"app\",\"maxlog\"), \"Unknown demapping method\"\n",
    "        self._method = method\n",
    "        self._hard_out = hard_out\n",
    "        self._num_bits_per_symbol = num_bits_per_symbol\n",
    "        self._with_prior = with_prior\n",
    "        num_points = int(2**num_bits_per_symbol)\n",
    "\n",
    "        # Array composed of binary representations of all symbols indices\n",
    "        a = np.zeros([num_points, num_bits_per_symbol])\n",
    "        for i in range(0, num_points):\n",
    "            a[i,:] = np.array(list(np.binary_repr(i, num_bits_per_symbol)),\n",
    "                              dtype=np.int16)\n",
    "\n",
    "        # Compute symbol indices for which the bits are 0 or 1\n",
    "        c0 = np.zeros([int(num_points/2), num_bits_per_symbol])\n",
    "        c1 = np.zeros([int(num_points/2), num_bits_per_symbol])\n",
    "        for i in range(num_bits_per_symbol-1,-1,-1):\n",
    "            c0[:,i] = np.where(a[:,i]==0)[0]\n",
    "            c1[:,i] = np.where(a[:,i]==1)[0]\n",
    "        self._c0 = tf.constant(c0, dtype=tf.int32) # Symbols with ith bit=0\n",
    "        self._c1 = tf.constant(c1, dtype=tf.int32) # Symbols with ith bit=1\n",
    "\n",
    "        if with_prior:\n",
    "            # Array of labels from {-1, 1} of all symbols\n",
    "            # [num_points, num_bits_per_symbol]\n",
    "            a = 2*a-1\n",
    "            self._a = tf.constant(a, dtype=dtype)\n",
    "\n",
    "        # Determine the reduce function for LLR computation\n",
    "        if self._method == \"app\":\n",
    "            self._reduce = tf.reduce_logsumexp\n",
    "        else:\n",
    "            self._reduce = tf.reduce_max\n",
    "\n",
    "    @property\n",
    "    def num_bits_per_symbol(self):\n",
    "        return self._num_bits_per_symbol\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if self._with_prior:\n",
    "            logits, prior = inputs\n",
    "        else:\n",
    "            logits = inputs\n",
    "\n",
    "        # Compute exponents\n",
    "        exponents = logits\n",
    "\n",
    "        # Gather exponents for all bits\n",
    "        # shape [...,n,num_points/2,num_bits_per_symbol]\n",
    "        exp0 = tf.gather(exponents, self._c0, axis=-1, batch_dims=0)\n",
    "        exp1 = tf.gather(exponents, self._c1, axis=-1, batch_dims=0)\n",
    "\n",
    "        # Process the prior information\n",
    "        if self._with_prior:\n",
    "            # Expanding `prior` such that it is broadcastable with\n",
    "            # shape [..., n or 1, 1, num_bits_per_symbol]\n",
    "            prior = sn.utils.expand_to_rank(prior, tf.rank(logits), axis=0)\n",
    "            prior = tf.expand_dims(prior, axis=-2)\n",
    "\n",
    "            # Expand the symbol labeling to be broadcastable with prior\n",
    "            # shape [..., 1, num_points, num_bits_per_symbol]\n",
    "            a = sn.utils.expand_to_rank(self._a, tf.rank(prior), axis=0)\n",
    "\n",
    "            # Compute the prior probabilities on symbols exponents\n",
    "            # shape [..., n or 1, num_points]\n",
    "            exp_ps = tf.reduce_sum(tf.math.log_sigmoid(a*prior), axis=-1)\n",
    "\n",
    "            # Gather prior probability symbol for all bits\n",
    "            # shape [..., n or 1, num_points/2, num_bits_per_symbol]\n",
    "            exp_ps0 = tf.gather(exp_ps, self._c0, axis=-1)\n",
    "            exp_ps1 = tf.gather(exp_ps, self._c1, axis=-1)\n",
    "\n",
    "        # Compute LLRs using the definition log( Pr(b=1)/Pr(b=0) )\n",
    "        # shape [..., n, num_bits_per_symbol]\n",
    "        if self._with_prior:\n",
    "            llr = self._reduce(exp_ps1 + exp1, axis=-2)\\\n",
    "                    - self._reduce(exp_ps0 + exp0, axis=-2)\n",
    "        else:\n",
    "            llr = self._reduce(exp1, axis=-2) - self._reduce(exp0, axis=-2)\n",
    "\n",
    "        if self._hard_out:\n",
    "            return sn.utils.hard_decisions(llr)\n",
    "        else:\n",
    "            return llr\n",
    "\n",
    "NUM_BITS_PER_SYMBOL = 4 # QPSK\n",
    "BLOCK_LENGTH = 8\n",
    "BATCH_SIZE = 2\n",
    "EBN0_DB = -10\n",
    "\n",
    "# Binary source\n",
    "binary_source = sn.utils.BinarySource()\n",
    "\n",
    "# Constellation\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "#constellation.show(figsize=(7,7));\n",
    "\n",
    "# Mapper and Demapper\n",
    "mapper = sn.mapping.Mapper(constellation=constellation)\n",
    "\n",
    "# AWGN channel\n",
    "awgn_channel = sn.channel.AWGN()\n",
    "\n",
    "bits = binary_source([BATCH_SIZE,BLOCK_LENGTH])\n",
    "#print('bits =\\n',bits)\n",
    "\n",
    "no = sn.utils.ebnodb2no(ebno_db=EBN0_DB,\n",
    "                        num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                        coderate=1.0)\n",
    "#print('no =',no)\n",
    "\n",
    "x = mapper(bits)\n",
    "#print('x =\\n',x)\n",
    "\n",
    "y = awgn_channel([x, no])\n",
    "\n",
    "# Reshape constellation points to [1,...1,num_points]\n",
    "points_shape = [1]*y.shape.rank + constellation.points.shape\n",
    "points = tf.reshape(constellation.points, points_shape)\n",
    "\n",
    "# Compute squared distances from y to all points\n",
    "# shape [...,n,num_points]\n",
    "squared_dist = tf.pow(tf.abs(tf.expand_dims(y, axis=-1) - points), 2)\n",
    "\n",
    "# Add a dummy dimension for broadcasting. This is not needed when no\n",
    "# is a scalar, but also does not do any harm.\n",
    "no = tf.expand_dims(no, axis=-1)\n",
    "\n",
    "# Compute exponents\n",
    "exponents = -squared_dist/no\n",
    "#print('exponents =\\n',exponents)\n",
    "\n",
    "logits2llrs = SymbolLogits2LLRs(\"app\",NUM_BITS_PER_SYMBOL)\n",
    "print('logits2llrs =\\n',logits2llrs)\n",
    "\n",
    "# if self._with_prior:\n",
    "#     llr = self._logits2llrs([exponents, prior])\n",
    "# else:\n",
    "llr = logits2llrs(exponents)\n",
    "\n",
    "# Reshape LLRs to [...,n*num_bits_per_symbol]\n",
    "out_shape = tf.concat([tf.shape(y)[:-1],\n",
    "                        [y.shape[-1] * \\\n",
    "                        constellation.num_bits_per_symbol]], 0)\n",
    "llr_reshaped = tf.reshape(llr, out_shape)\n",
    "\n",
    "# return llr_reshaped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bits2symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bits = tf.Tensor(\n",
      "[[[[[0. 0. 1. 1. 1. 0.]\n",
      "    [1. 0. 0. 0. 1. 1.]\n",
      "    [0. 0. 1. 0. 1. 0.]\n",
      "    [1. 0. 0. 1. 1. 1.]\n",
      "    [0. 1. 0. 1. 1. 0.]]\n",
      "\n",
      "   [[0. 0. 0. 1. 1. 0.]\n",
      "    [0. 0. 1. 0. 1. 1.]\n",
      "    [0. 0. 0. 0. 1. 0.]\n",
      "    [0. 1. 0. 0. 0. 0.]\n",
      "    [1. 0. 1. 0. 0. 1.]]\n",
      "\n",
      "   [[0. 0. 0. 1. 0. 0.]\n",
      "    [1. 1. 1. 1. 1. 1.]\n",
      "    [1. 0. 1. 1. 0. 1.]\n",
      "    [1. 1. 0. 0. 1. 1.]\n",
      "    [0. 0. 1. 1. 1. 0.]]\n",
      "\n",
      "   [[0. 0. 1. 0. 0. 1.]\n",
      "    [0. 1. 0. 0. 0. 0.]\n",
      "    [0. 0. 0. 1. 0. 0.]\n",
      "    [1. 1. 1. 1. 0. 0.]\n",
      "    [0. 0. 0. 1. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 1. 1. 0. 1. 0.]\n",
      "    [1. 0. 0. 0. 1. 1.]\n",
      "    [1. 1. 1. 0. 1. 1.]\n",
      "    [1. 1. 0. 0. 1. 1.]\n",
      "    [1. 0. 1. 1. 0. 0.]]\n",
      "\n",
      "   [[0. 1. 1. 0. 0. 0.]\n",
      "    [1. 0. 0. 1. 1. 0.]\n",
      "    [0. 0. 0. 1. 1. 1.]\n",
      "    [0. 1. 1. 0. 1. 0.]\n",
      "    [0. 1. 1. 1. 1. 0.]]\n",
      "\n",
      "   [[1. 0. 0. 1. 0. 1.]\n",
      "    [0. 1. 0. 0. 0. 1.]\n",
      "    [1. 0. 0. 1. 0. 1.]\n",
      "    [1. 0. 1. 0. 0. 0.]\n",
      "    [1. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "   [[1. 1. 1. 0. 0. 0.]\n",
      "    [0. 0. 0. 0. 0. 0.]\n",
      "    [1. 1. 1. 0. 0. 0.]\n",
      "    [1. 1. 1. 1. 1. 1.]\n",
      "    [0. 1. 0. 1. 0. 1.]]]\n",
      "\n",
      "\n",
      "  [[[1. 1. 1. 0. 0. 1.]\n",
      "    [0. 0. 0. 0. 0. 0.]\n",
      "    [1. 1. 0. 1. 0. 1.]\n",
      "    [1. 1. 0. 0. 0. 1.]\n",
      "    [1. 1. 1. 0. 0. 1.]]\n",
      "\n",
      "   [[1. 0. 1. 0. 0. 0.]\n",
      "    [0. 0. 0. 1. 1. 1.]\n",
      "    [0. 0. 0. 1. 0. 1.]\n",
      "    [0. 1. 1. 1. 0. 0.]\n",
      "    [1. 1. 1. 1. 1. 0.]]\n",
      "\n",
      "   [[0. 1. 0. 1. 0. 0.]\n",
      "    [0. 1. 0. 1. 1. 0.]\n",
      "    [1. 0. 1. 0. 0. 1.]\n",
      "    [1. 0. 0. 1. 1. 0.]\n",
      "    [0. 0. 0. 1. 1. 1.]]\n",
      "\n",
      "   [[0. 0. 0. 0. 1. 0.]\n",
      "    [0. 0. 0. 1. 1. 1.]\n",
      "    [1. 1. 1. 0. 0. 0.]\n",
      "    [1. 1. 0. 1. 1. 1.]\n",
      "    [0. 0. 1. 0. 0. 0.]]]]\n",
      "\n",
      "\n",
      "\n",
      " [[[[1. 1. 0. 1. 0. 1.]\n",
      "    [0. 1. 1. 1. 1. 1.]\n",
      "    [0. 1. 1. 1. 0. 1.]\n",
      "    [0. 0. 0. 1. 0. 0.]\n",
      "    [0. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "   [[0. 0. 1. 1. 1. 0.]\n",
      "    [0. 0. 0. 1. 0. 1.]\n",
      "    [1. 1. 1. 0. 1. 0.]\n",
      "    [0. 0. 1. 0. 0. 0.]\n",
      "    [1. 1. 0. 1. 0. 0.]]\n",
      "\n",
      "   [[0. 1. 0. 1. 1. 0.]\n",
      "    [1. 0. 1. 1. 1. 0.]\n",
      "    [1. 0. 1. 1. 0. 0.]\n",
      "    [1. 1. 1. 1. 0. 0.]\n",
      "    [1. 0. 1. 1. 0. 0.]]\n",
      "\n",
      "   [[0. 1. 1. 1. 0. 1.]\n",
      "    [1. 0. 1. 1. 0. 0.]\n",
      "    [1. 1. 0. 0. 1. 0.]\n",
      "    [1. 0. 0. 0. 1. 0.]\n",
      "    [0. 1. 0. 1. 0. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 0. 0. 0.]\n",
      "    [0. 1. 1. 1. 0. 0.]\n",
      "    [0. 0. 0. 1. 1. 1.]\n",
      "    [0. 0. 1. 0. 0. 0.]\n",
      "    [1. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 1. 0. 0. 1. 0.]\n",
      "    [1. 0. 1. 1. 0. 0.]\n",
      "    [0. 0. 0. 1. 0. 1.]\n",
      "    [0. 0. 1. 0. 1. 1.]\n",
      "    [1. 0. 0. 0. 0. 1.]]\n",
      "\n",
      "   [[0. 1. 0. 0. 1. 0.]\n",
      "    [0. 1. 0. 0. 0. 1.]\n",
      "    [0. 0. 1. 0. 1. 0.]\n",
      "    [1. 0. 0. 1. 0. 0.]\n",
      "    [1. 0. 1. 0. 0. 0.]]\n",
      "\n",
      "   [[0. 0. 1. 0. 0. 0.]\n",
      "    [0. 0. 0. 1. 1. 0.]\n",
      "    [0. 1. 0. 1. 0. 1.]\n",
      "    [1. 0. 0. 0. 1. 0.]\n",
      "    [1. 1. 1. 1. 1. 0.]]]\n",
      "\n",
      "\n",
      "  [[[0. 0. 0. 1. 1. 1.]\n",
      "    [0. 1. 1. 0. 0. 0.]\n",
      "    [1. 0. 1. 1. 0. 1.]\n",
      "    [0. 1. 0. 0. 1. 0.]\n",
      "    [1. 0. 0. 0. 1. 0.]]\n",
      "\n",
      "   [[1. 1. 1. 1. 0. 0.]\n",
      "    [0. 0. 1. 1. 0. 0.]\n",
      "    [1. 1. 1. 0. 1. 1.]\n",
      "    [1. 1. 1. 0. 0. 0.]\n",
      "    [1. 0. 0. 1. 0. 0.]]\n",
      "\n",
      "   [[1. 0. 0. 0. 1. 1.]\n",
      "    [1. 0. 0. 1. 0. 0.]\n",
      "    [1. 1. 1. 0. 0. 1.]\n",
      "    [1. 1. 0. 1. 0. 1.]\n",
      "    [0. 1. 0. 0. 0. 0.]]\n",
      "\n",
      "   [[1. 1. 0. 1. 0. 1.]\n",
      "    [0. 0. 0. 0. 1. 1.]\n",
      "    [0. 0. 1. 1. 1. 0.]\n",
      "    [1. 0. 0. 1. 0. 1.]\n",
      "    [1. 1. 0. 1. 1. 1.]]]]], shape=(2, 3, 4, 5, 6), dtype=float32)\n",
      "bits.ndim = 5\n",
      "new_shape = [-1, 3, 4, 5, 1, 4]\n",
      "bits.shape[1:-1] (3, 4, 5)\n",
      "bits.shape[-1]= 6\n",
      "symbols = tf.Tensor(\n",
      "[[[[[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[1 0 0 0]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[1 1 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 1 1 0]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 1 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 0 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]]]]], shape=(3, 3, 4, 5, 1, 4), dtype=int32)\n",
      "symbols = tf.Tensor(\n",
      "[[[[[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[1 0 0 0]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[1 1 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 1 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 1 1 0]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 1 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 1 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 1 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]]]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " [[[[[0 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 0]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 0 0 1]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[1 0 1 0]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 1 0 0]]\n",
      "\n",
      "    [[1 0 1 0]]\n",
      "\n",
      "    [[0 0 0 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[0 0 0 1]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]]\n",
      "\n",
      "\n",
      "   [[[0 0 0 1]]\n",
      "\n",
      "    [[1 1 0 1]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]]\n",
      "\n",
      "\n",
      "   [[[0 0 1 0]]\n",
      "\n",
      "    [[1 0 0 0]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 0 0]]\n",
      "\n",
      "    [[0 0 1 1]]]]\n",
      "\n",
      "\n",
      "\n",
      "  [[[[0 0 1 1]]\n",
      "\n",
      "    [[1 0 1 1]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 0 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 0 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[0 1 1 1]]]\n",
      "\n",
      "\n",
      "   [[[0 1 0 1]]\n",
      "\n",
      "    [[0 1 0 0]]\n",
      "\n",
      "    [[0 0 1 1]]\n",
      "\n",
      "    [[0 1 0 1]]\n",
      "\n",
      "    [[0 0 0 0]]]\n",
      "\n",
      "\n",
      "   [[[1 1 0 0]]\n",
      "\n",
      "    [[1 1 1 0]]\n",
      "\n",
      "    [[1 0 0 1]]\n",
      "\n",
      "    [[0 1 1 1]]\n",
      "\n",
      "    [[0 1 1 1]]]]]], shape=(3, 3, 4, 5, 1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Imports & Basics\n",
    "\n",
    "# Import TensorFlow and NumPy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "\n",
    "# For plotting\n",
    "%matplotlib inline\n",
    "# also try %matplotlib widget\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "from sionna.utils.misc import hard_decisions\n",
    "from sionna.utils.metrics import compute_ber, compute_ser\n",
    "\n",
    "def bits2symbol(bits,num_bits_per_symbol):\n",
    "    new_shape = [-1] + bits.shape[1:-1].as_list() + \\\n",
    "            [int(bits.shape[-1] / NUM_BITS_PER_SYMBOL),\n",
    "                NUM_BITS_PER_SYMBOL]\n",
    "    #print('new_shape =',new_shape)\n",
    "    #print('bits.shape[1:-1]',bits.shape[1:-1])\n",
    "    print('bits.shape[-1]=',bits.shape[-1])\n",
    "\n",
    "    symbols = tf.cast(tf.reshape(bits, new_shape), tf.int32)\n",
    "    print('symbols =',symbols)\n",
    "    return symbols\n",
    "\n",
    "\n",
    "CODERATE = 0.5\n",
    "n = 16\n",
    "k = int(n*CODERATE)\n",
    "\n",
    "NUM_BITS_PER_SYMBOL = 4 # QPSK\n",
    "BLOCK_LENGTH = k\n",
    "BATCH_SIZE = 2 # How many examples are processed by Sionna in parallel\n",
    "EBN0_DB_MIN = -10.0 # Minimum value of Eb/N0 [dB] for simulations\n",
    "EBN0_DB_MAX = 10.0 # Maximum value of Eb/N0 [dB] for simulations\n",
    "\n",
    "# Binary source\n",
    "binary_source = sn.utils.BinarySource()\n",
    "\n",
    "# Constellation\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "\n",
    "#bits = binary_source([BATCH_SIZE,BLOCK_LENGTH])\n",
    "bits = binary_source([2,3,4,5,6])\n",
    "print('bits =',bits)\n",
    "print('bits.ndim =',bits.ndim)\n",
    "\n",
    "symbols = bits2symbol(bits,NUM_BITS_PER_SYMBOL)\n",
    "print('symbols =',symbols)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
