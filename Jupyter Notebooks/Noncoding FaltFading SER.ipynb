{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-22 12:17:07.916277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs available : 0\n",
      "Data Group 0\n",
      "Processing...\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# For the implementation of the Keras models\n",
    "from tensorflow import keras\n",
    "from keras import Model\n",
    "# for performance measurements\n",
    "import time\n",
    "\n",
    "# GPU Configuration and Imports\n",
    "# Configure the notebook to use only a single GPU and allocate only as much memory as needed\n",
    "# For more details, see https://www.tensorflow.org/guide/gpu\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "if gpus:\n",
    "    gpu_num = 0 # Number of the GPU to be used\n",
    "    try:\n",
    "        tf.config.set_visible_devices(gpus[gpu_num], 'GPU')\n",
    "        print('Only GPU number', gpu_num, 'used.')\n",
    "        tf.config.experimental.set_memory_growth(gpus[gpu_num], True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "# Import Sionna\n",
    "try:\n",
    "    import sionna as sn\n",
    "except ImportError as e:\n",
    "    # Install Sionna if package is not already installed\n",
    "    import os\n",
    "    os.system(\"pip install sionna\")\n",
    "    import sionna as sn\n",
    "    \n",
    "from sionna.utils import BinarySource, QAMSource, ebnodb2no, compute_ser, compute_ber, PlotBER\n",
    "from sionna.channel import FlatFadingChannel, KroneckerModel\n",
    "from sionna.channel.utils import exp_corr_mat\n",
    "from sionna.mimo import lmmse_equalizer, zf_equalizer\n",
    "from sionna.mapping import SymbolDemapper, Mapper, Demapper\n",
    "from sionna.fec.ldpc.encoding import LDPC5GEncoder\n",
    "from sionna.fec.ldpc.decoding import LDPC5GDecoder\n",
    "from sionna.utils.misc import hard_decisions\n",
    "from sionna.utils.metrics import compute_ber\n",
    "from xyDIP import DeepImagePrior\n",
    "from complex2real import Complex2Real\n",
    "\n",
    "k = 512 # Block Length:512\n",
    "NUM_TX_ANT = 4 # Transmit Antennas: 4\n",
    "NUM_RX_ANT = 4 # Receive Antennas: 4\n",
    "NUM_BITS_PER_SYMBOL = 4 # Mapping: 16QAM\n",
    "BATCH_SIZE = 128 # Number of Parallelly Processed Batches: 128\n",
    "EBN0_DB_MIN = -5 # Minimum Eb/N0 (dB): -5\n",
    "EBN0_DB_MAX = 20 # Maximum Eb/N0 (dB): 20\n",
    "NUM_EBN0_POINTS = 11 # EBNO Points: 11\n",
    "NUM_DATA_GROUP = 5 # Number of Data Group: 5\n",
    "\n",
    "snrs = np.linspace(EBN0_DB_MIN,EBN0_DB_MAX,NUM_EBN0_POINTS)\n",
    "bers = []\n",
    "sers_zf = np.empty((NUM_DATA_GROUP, NUM_EBN0_POINTS))\n",
    "sers_lmmse = np.empty((NUM_DATA_GROUP, NUM_EBN0_POINTS))\n",
    "sers_dip = np.empty((NUM_DATA_GROUP, NUM_EBN0_POINTS))\n",
    "\n",
    "# Binary Source\n",
    "binary_source = sn.utils.BinarySource()\n",
    "\n",
    "# Constellation\n",
    "constellation = sn.mapping.Constellation(\"qam\", NUM_BITS_PER_SYMBOL)\n",
    "#constellation.show(figsize=(7,7));\n",
    "\n",
    "# Mapper and Demapper\n",
    "mapper = sn.mapping.Mapper(constellation=constellation)\n",
    "# The demapper uses the same constellation object as the mapper\n",
    "demapper = sn.mapping.Demapper(\"app\", constellation=constellation)\n",
    "\n",
    "# AWGN Channel\n",
    "awgn_channel = sn.channel.AWGN()\n",
    "\n",
    "# Flat Fading Channel\n",
    "# To simulate transmissions over an i.i.d. Rayleigh fading channel. The channel will also add AWGN with variance `no`.\n",
    "flatfading_channel = FlatFadingChannel(NUM_TX_ANT, NUM_RX_ANT, add_awgn=True, return_channel=True)\n",
    "\n",
    "# SymbolDemapper\n",
    "symbol_demapper = SymbolDemapper(\"qam\", NUM_BITS_PER_SYMBOL, hard_out=True)\n",
    "\n",
    "# Deep Image Prior\n",
    "dip = DeepImagePrior(num_rx_ant=NUM_RX_ANT,\n",
    "                    num_tx_ant=NUM_TX_ANT,      # Number of transmitted symbol in real domain\n",
    "                    M=16,              # Modulation order, 4 for 4QAM, 16 for 16QAM; 16QAM: 4 bits per symbol\n",
    "                    iteration=100,    # Number of max iterations used for DIP: 100\n",
    "                    LR=0.01,          # Learning rate,  typically set to 0.01\n",
    "                    buffer_size=30,   # Iterations stored,  typically set to 30\n",
    "                    threshold=0.001,  # Threshold of DIP stop,, typically set to 0.001\n",
    "                    stop=True)        # True\n",
    "\n",
    "# Complex2Real\n",
    "c2r = Complex2Real(NUM_RX_ANT,NUM_TX_ANT)\n",
    "\n",
    "for i in range(0, NUM_DATA_GROUP):\n",
    "\n",
    "    print('Data Group {}'.format(i))\n",
    "    print('Processing...')\n",
    "\n",
    "    b = binary_source([BATCH_SIZE,NUM_TX_ANT,k])\n",
    "    # print('b shape =',b.shape)\n",
    "\n",
    "    x = mapper(b)\n",
    "    # print('x shape =',x.shape)\n",
    "    # print('x =',x)\n",
    "\n",
    "    shape = tf.shape(x)\n",
    "    x_reshape = tf.reshape(x,[-1, NUM_TX_ANT])\n",
    "    # print('x reshape =',x_reshape.shape)\n",
    "    # print('x_reshape =',x_reshape)\n",
    "\n",
    "    # Tx_Symbols = bits2symbol(b,NUM_BITS_PER_SYMBOL)\n",
    "    # print('Tx_Symbols =',Tx_Symbols)\n",
    "    \n",
    "    j = 0\n",
    "    \n",
    "    for EBN0_DB in np.linspace(EBN0_DB_MIN,EBN0_DB_MAX,NUM_EBN0_POINTS):\n",
    "        \n",
    "        # print('EBN0_DB =',EBN0_DB)\n",
    "\n",
    "        no = sn.utils.ebnodb2no(ebno_db=EBN0_DB,\n",
    "                                num_bits_per_symbol=NUM_BITS_PER_SYMBOL,\n",
    "                                coderate=1.0) # Coderate set to 1 as we do uncoded transmission here\n",
    "        # print('no =',no)\n",
    "\n",
    "        x_ind = symbol_demapper([x, no])\n",
    "        # print('x_ind.shape =',x_ind.shape)\n",
    "        # print('x_ind =',x_ind)\n",
    "\n",
    "        # y and h are the Channel Output and Channel Realizations, respectively.\n",
    "        y, h = flatfading_channel([x_reshape, no])\n",
    "        # print('h.shape =\\n',h.shape)\n",
    "        # print('y.shape =\\n',y.shape)\n",
    "\n",
    "        s = tf.cast(no*tf.eye(NUM_RX_ANT, NUM_RX_ANT), y.dtype)\n",
    "\n",
    "        x_hat_zf, no_eff_zf = zf_equalizer(y, h, s)\n",
    "        # print('x_hat_zf.shape =',x_hat_zf.shape)\n",
    "        # print('x_hat_zf =',x_hat_zf)\n",
    "        x_hat_lmmse, no_eff_lmmse = lmmse_equalizer(y, h, s)\n",
    "        # print('x_hat_lmmse.shape =',x_hat_lmmse.shape)\n",
    "\n",
    "        X_inCH_real,H_real,Y_real = c2r.C2R(x_reshape,h,y)\n",
    "        # print('X_inCH_real =',X_inCH_real)\n",
    "        # print('max =',np.max(X_inCH_real))\n",
    "        # print('H_real =',H_real)\n",
    "        # print('Y_real =',Y_real)\n",
    "\n",
    "        x_dip_ay,num_stop_point = dip.DIP(Y_real,H_real)\n",
    "        # print('x_dip_ay.shape =',x_dip_ay.shape)\n",
    "        # print('x_dip =',x_dip_ay)\n",
    "        # print('num_stop_point =',num_stop_point)\n",
    "        x_dip_ay_real_part,x_dip_ay_imag_part = tf.split(x_dip_ay, num_or_size_splits=2, axis=2)\n",
    "        # print('sum_real =',sum_real)\n",
    "        # print('sum_imag =',sum_imag)\n",
    "        x_hat_dip = tf.squeeze(tf.squeeze(tf.complex(x_dip_ay_real_part,x_dip_ay_imag_part),axis=-1),axis=-1)\n",
    "        # print('x_hat_dip =',x_hat_dip)\n",
    "\n",
    "        x_hat_zf = tf.reshape(x_hat_zf, shape)\n",
    "        # print('x_hat_zf.shape =',x_hat_zf.shape)\n",
    "        x_hat_lmmse = tf.reshape(x_hat_lmmse, shape)\n",
    "        # print('x_hat_lmmse.shape =',x_hat_lmmse.shape)\n",
    "        x_hat_dip = tf.reshape(x_hat_dip, shape)\n",
    "        # print('x_hat_lmmse.shape =',x_hat_lmmse.shape)\n",
    "\n",
    "        # no_eff_zf = tf.reshape(no_eff_zf, shape)\n",
    "        # no_eff_lmmse = tf.reshape(no_eff_lmmse, shape)\n",
    "\n",
    "        # llr_zf = demapper([x_hat_zf, no_eff_zf])\n",
    "        # b_hat_zf = decoder(llr_zf)\n",
    "\n",
    "        x_ind_hat_zf = symbol_demapper([x_hat_zf, no])\n",
    "        # print('x_ind_hat_zf.shape =',x_ind_hat_zf.shape)\n",
    "        # print('x_ind_hat_zf =',x_ind_hat_zf)\n",
    "        x_ind_hat_lmmse = symbol_demapper([x_hat_lmmse, no])\n",
    "        # print('x_ind_hat_lmmse.shape =',x_ind_hat_lmmse.shape)\n",
    "        x_ind_hat_dip = symbol_demapper([tf.cast(x_hat_dip, dtype=tf.complex64), no])\n",
    "        # print('x_ind_hat_lmmse.shape =',x_ind_hat_lmmse.shape)\n",
    "\n",
    "        ser_zf = compute_ser(x_ind, x_ind_hat_zf)\n",
    "        ser_lmmse = compute_ser(x_ind, x_ind_hat_lmmse)\n",
    "        ser_dip = compute_ser(x_ind, x_ind_hat_dip)\n",
    "        sers_zf[i, j] = ser_zf\n",
    "        sers_lmmse[i, j] = ser_lmmse\n",
    "        sers_dip[i, j] = ser_dip\n",
    "\n",
    "        j = j+1\n",
    "    print('Done')\n",
    "\n",
    "sers_zf_mean = np.mean(sers_zf, axis=0)\n",
    "sers_lmmse_mean = np.mean(sers_lmmse, axis=0)\n",
    "sers_dip_mean = np.mean(sers_dip, axis=0)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.axes().set_aspect(1)\n",
    "plt.grid(True)\n",
    "plt.title('Flat-Fading Channel Constellation', fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel('REAL', fontsize=10)\n",
    "plt.ylabel('IMAG', fontsize=10)\n",
    "plt.scatter(tf.math.real(x), tf.math.imag(x), s=16, c='b', label='TX')\n",
    "plt.scatter(tf.math.real(x_hat_zf), tf.math.imag(x_hat_zf), s=16, c='y', label='ZF')\n",
    "plt.scatter(tf.math.real(x_hat_lmmse), tf.math.imag(x_hat_lmmse), s=16, c='g', label='LMMSE')\n",
    "plt.scatter(tf.math.real(x_hat_dip), tf.math.imag(x_hat_dip), s=16, c='r', label='DIP')\n",
    "plt.legend(loc='lower left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.figure(2)\n",
    "title = \"SER: Noncoding MIMO Falt-Fading with ZF, MMSE, DIP Equalizer\"\n",
    "xlabel = \"$E_b/N_0$ (dB)\"\n",
    "ylabel = \"SER (log)\"\n",
    "plt.title(title, fontsize=12)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "plt.xlabel(xlabel, fontsize=10)\n",
    "plt.ylabel(ylabel, fontsize=10)\n",
    "plt.grid(which=\"both\")\n",
    "plt.semilogy(snrs, sers_zf_mean, 'b', label='ZF')\n",
    "plt.semilogy(snrs, sers_lmmse_mean, 'g', label='LMMSE')\n",
    "plt.semilogy(snrs, sers_dip_mean, 'r', label='DIP')\n",
    "plt.legend(loc='lower left', fontsize=8)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sionna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
